29.11.12
* Add the auto complete to the search boxes *
* Re-test the advanced search in the interface *
* Add icons to the interface *
* Draw circle on GoogleMap for geospatial search circle *

03.11.10
21.05.12
04.07.12

* Enrich the geospatial data with cities and countries - ?
   1) First with the country data within the data - 1
   2) With Google or NavTec data - 2
* Add search results tracking
   Analysis of the search results with resulting reports, top queries, results from queries etc. - 3
* Taxonomy, stemming and synonyms, perhaps other language results too - 1
* Persist the results of the rules along with the action - 1

Sub Total: 7

* Implementation of updating the index with deltas from the client - 3
* Move the data source to a central location for the table indexables - 4 hours
* Do a configuration page that lets the user see the configuration per index and indexable
* System test with report - 5

Sub total: 9 1/2

Total: 16 1/2


Bugs:

Done:
* Cluster - 3 *
* Finish the clustering support - 3 *
* Implement the synchronization between the servers - 1 *
* Add the other parsers - 3 *
* Performance - 1 *
* Large volumes - 1 *
* Add the file and url visitors - 5 *
* Multiple data sources - 1 *
* Testing - 7 *
* Cluster testing - 1 *
* Wiki/documentation/JavaDoc - 3 *
* Data generator two with files - 1/2 *
* Reconfigure the field names for the monitoring - 4 hours (Michael) *
* Rules engine for the decision - 3 (Michael) *
* New data structure, perhaps patients in a hospital - 1 *
* Create automated integration tests in different environments - 3 *
* Add the possibility for persistence in JPA - 1 *
* Stress and load test the web service - 1 *
* Back up action - 4 hours *
* Implement distance sorting, i.e. given the co-ordinates of the originator and the co-ordinates
   of the results sort them from the closest to the furthest from the orginator - 3 (Michael) *
* UI - client front end for search and monitoring - 3 1/2 (Michael)
   Logging on a topic for the gui to collect for the whole cluster is an idea
   2) Administration to start an in index - 1
   3) Monitoring page to see what the servers are doing - 1 *
   1) Search page for the documentation - 1 *
   4) Search page to search the individual indexes - 1 *
* Add a disk full check, in the ui and in the handlers - 1 *
* Expanded monitoring - 3 *
* Test the serialization of Cyrillic and other character sets during the search
   and the de-serialization of the results - 1 hour *
* Add the web service urls to the servers page - 1 *
* Change the search url from the servlet to the web service ? *
* Add a shutdown that closes everything when the application is being un-deployed 
   so the new application starts the application context cleanly - 3 *
* Verify why the cluster is not acting as expected ? *
* Implement paging in the search results - 1 *
* Write a script to automate the deploy to the local repository for drivers etc. - 1 *
* Verify why the two indexes have the same timestamp *
* Get the start time for each action and the end time and persist it *
* Add strategies for indexing large files like zips - 3 *
* Do an action log page with all the actions that were performed and the results from the rules
* Add the actions to the database and a page to see them with the time stamps for all the servers.
* Documentation - 1
   3) On Wiki Pedia - 4 hours *
   4) On various Java sites - 4 hours *
   2) Migrate the documentation from docs to the application - 4 hours *
   1) On the Google code site - 4 hours * 
* Test writing the index across the network, and reading, perhaps a stress test too - 1


Nice to have:
* Add a database tester to re-connect if the database becomes disconnected - 4 hours
* Add changes in the data to results? i.e. save the previous data and in the results - 3
   offer a series of documents representing how they changed over the 
   indexes/periods, i.e. shallow deltas.
* Add a shutdown hook for the Spring objects that need to be closed when the context - 3
  gets refreshed. This can aid in the dynamic configuration.
* Security - 3
* Spelling checking, done just need to migrate the code from the old project. - 1
* Phonetic search - 3
* Result clustering - 3
* Reporting (e.g. Google Analytics) - 1
* Add a maximum documents per server parameter - 1
* Performance benchmarking - 3
* SMS messages for monitoring actions - 2
* Secure datasource connections - 3
* Update the boosting according to the usage clicks, i.e. ranking - 3

Done:
* Delete the database file on startup locally *
* Add a reporting module that checks the database connection, the indexes in the different contexts and sends
   the result to a mail address. Perhaps some profiling information, etc. *
* Add a configuration for a file system *
* Add a configuration for a mail account *
* Test the parsers *
* Add sorting to the multi and single searchers *
* Is there automatically language support, i.e. are languages handled transparently? Chinese,
   Russian, other scripting languages? It seems that way in fact. Certainly Russsian and Arabic 
   are handled by Lucene automatically. (Michael)

Won't Do:
* Boosting
* Servlet/Ejb - 1?
* Add slave to run on the client machine and transfer data to the master running 
   on our servers as a hosting service.
* Add possibility to have index in memory so as not to write on the file system - 3
* Add facets to the search possibilities - 5
* Re-factor common code to tools module - 1 *
* Secure web service and servlet - 3 *
* Add a page with Cpu and system sampling data using Sigar API or JavaSysMon.
   1) Cpu tree
   2) Disk usage
   3) Ram usage
   4) Up time
   5) Open files

Notes:
==Some features of Ikube:==

 * Cost â‚¬0.00
 * Auto cpu load tuning
 * Embeddable, theoretically
 * Multi-threaded per Jvm and cluster-able
 * High volume, 100 000 000 documents/records++
 * Highly configurable and customizable
 * Spelling checking, any language that you care to add, English included
 * Spelling suggestions, can add any language you want, comes with English of course
 * Geo-spatial search, i.e. results sorted by distance
 * Geo-spatial indexing, i.e. if you have the address, Ikube will find the co-ordinates, enrich your data with it, and you can have the results sorted by distance
 * Multi field search, i.e. span queries, and the data is segmented into individual fields. This means that applications can search for 'name = xxx' and 'birthdate < yyy' and 'country in (z,z,z)' sort by distance from point (-33.9693580, 18.4622110), which is Cape Town as it turns out.
 * Of course the usual things like fuzzy search, wildcard and everything offered by Lucene
 * Results in pure XML or Json via a (secured by Spring) rest web service
 * Of course open source is always welcome in the enterprise environment
 * Queries per minute in a single thread, quad core Dell rack 310, around 300. Note that this is distance(spatial) sorted queries, with 7 500 000 records, so you can multiply that by 3 for a normal query in the same size document set, we all know what Lucene can do no?
 * Flexible continuous crawl schedules
 * Multi lingual or course, i.e. the indexes can be any language, like French for example
 * Administration notification by email, and soon by text message, you will have to get your own account at [http://www.twilio.com/ Twilio]
 * Indexing and search monitoring and performance
 * Deploy stand alone, in a server(any one, Tomcat, JBoss...), as a Jar, War, Ear
 * Automatic clustering via UDP broardcasting, i.e. you start another stand alone and they will find each other and automatically synchronize with each other
 * Any database, tested extensively on Db2, H2, Oracle and Postgres
 * Any operating system, tested on Linux and a little on Windows
 * Spring enabled, in fact based on Spring and Maven
 * Examples for iterating over results in EL, currently in AngularJS   